{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1. Data-driven Approach\n",
    "# 4.2. Loss Function\n",
    "## 4.2.1. MSE\n",
    "\n",
    "- $E = \\frac{1}{2}\\sum_{k}^{}{(y_{k} - t_{k}) ^2}$\n",
    "\n",
    "    - ${y_{k}}$는 신경망의 출력, ${t_{k}}$는 정답 레이블, k는 데이터의 차원수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Cross Entropy\n",
    "\n",
    "- $E=-\\sum_{k}{t_{k}}{logy_{k}}$\n",
    "\n",
    "   - ${y_{k}}$는 신경망의 확률 출력값, ${t_{k}}$는 라벨의 one-hot-encoding 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding 되어있지 않을 때\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 Mini-Batch Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets.mnist import load_data\n",
    "\n",
    "(X_train, y_train), (X_test,y_test) = load_data()\n",
    "X_train, X_test = X_train.reshape(60000,-1) / 255, X_test.reshape(10000, -1) / 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train.reshape(-1, 1))\n",
    "y_train, y_test = enc.transform(y_train.reshape(-1, 1)).toarray(), enc.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(x, y, batch_size):\n",
    "    mask = np.random.choice(x.shape[0], batch_size)\n",
    "    return x[mask], y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "tmp1,tmp2 = mini_batch(X_train, y_train, batch_size = 128)\n",
    "print(tmp1.shape, tmp2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 Cross-Entropy for Mini-Batch Learning\n",
    "- $E=-\\frac{1}{N}\\sum_{n}\\sum_{k}{t_{nk}}{logy_{nk}}$\n",
    "\n",
    "    - \"평균 손실 함수\"를 구하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim != 1:\n",
    "        t = t.flatten()\n",
    "        y = y.flatten()\n",
    "        return -np.sum(t*np.log(y + delta) / y.shape[0]) \n",
    "    else:\n",
    "        return -np.sum(t*np.log(y + delta)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def softmax(x):\n",
    "    nom = np.exp(x - np.max(x, axis = 1)[:, np.newaxis])\n",
    "    denom = np.sum(nom, axis = 1)[:, np.newaxis]\n",
    "    return nom / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.normal(size = (input_size, hidden_size))\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.normal(size = (hidden_size, output_size))\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2, b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        accuracy = np.sum(y == t).astype(np.float) / x.shape[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50) (50, 10)\n",
      "(50,) (10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['W2'].shape)\n",
    "print(net.params['b1'].shape, net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "iter_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss, train_acc, test_acc = [], [], []\n",
    "\n",
    "for i in range(iter_num):\n",
    "    x_batch, y_batch = mini_batch(X_train, y_train, batch_size)\n",
    "    grad = net.numerical_gradient(x_batch, y_batch)\n",
    "    \n",
    "    for key in net.params.keys():\n",
    "        net.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    if i % iter_num / batch_size == 0: # iter_num_per_epoch\n",
    "        loss = net.loss(x_batch, y_batch)\n",
    "        acc1, acc2 = net.accuracy(X_train, y_train), net.accuracy(X_test, y_test)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc1)\n",
    "        test_acc.append(acc2)\n",
    "\n",
    "        print(i,\":\", \"loss, train acc, test acc |\" , str(loss) +',', str(acc1) +',', str(acc2))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
